{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiSDqEU06NjA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "gy2r5u8FL7fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params():\n",
        "  W1 = np.random.rand(10,)-0.5 #10 neurons,number of rows in train set vals between -0.5 and 0.5\n",
        "  b1 = np.random.rand(10,1) - 0.5\n",
        "  W2 = np.random.rand(10,10)-0.5 #10 neurons,number of rows in train set vals between -0.5 and 0.5\n",
        "  b2 = np.random.rand(10,1) - 0.5\n",
        "  return W1,b1,W2,b2\n",
        "\n",
        "\n",
        "def forward_prop(W1,b1,W2,b2):\n",
        "  Z1 = W1.dot(X) + b1\n",
        "  A1 = sigmoid(Z1)\n",
        "  Z2 = W2.dot(A1) + b2\n",
        "  A2 = softmax(Z2)\n",
        "  return Z1,A1,Z2,A2\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  return (1/(1+np.exp(-x))*(1-(1/(1+np.exp(-x)))))\n",
        "\n",
        "def backward_prop(Z1,A1,Z2,A2,W2,X,y):\n",
        "  dZ2 = A2-y\n",
        "  dW2 = 1/m*dZ2.dot(A1.T)\n",
        "  db2 = 1/m*np.sum(dZ2,2)\n",
        "  dZ1 = W2.T.dot(dZ2) * deriv_sigmoid(Z2)\n",
        "  dW1 = 1/m*dZ1.dot(X.T)\n",
        "  db1 = 1/m*np.sum(dZ1,2)\n",
        "  return dW1,db1,dW2,db2\n",
        "\n",
        "def update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,W1,W2,alpha):\n",
        "  W1 = W1 - alpha* dW1\n",
        "  b1 = b1 - alpha* db1\n",
        "  W2 = W2 - alpha* dW2\n",
        "  b2 = b2 - alpha* db2\n",
        "  return W1,b1,W2,b2\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    return exp(x)/np.sum(exp(x))\n",
        "\n",
        "def get_predictions(A2):\n",
        "  return np.argmax(A2,0)\n",
        "\n",
        "def get_accuracy(predictions,y):\n",
        "  return np.sum(predictions==y)/y.size\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "ERNJTSDjEIQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X,y,iterations,alpha):\n",
        "  W1,b1,W2,b2 = init_params()\n",
        "  for i in range(iterations):\n",
        "    Z1,A1,Z2,A2 = forward_prop(W1,b1,W2,b2)\n",
        "    dW1,db1,dW2,db2 = backward_prop(Z1,A1,Z2,A2,W2,y)\n",
        "    W1,b1,W2,b2 = update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,W1,W2,alpha)\n",
        "    if i%50 == 0:\n",
        "      print(\"iteration:\"i)\n",
        "      print(\"accuracy:\"get_accuracy(get_predictions(A2),y))\n",
        "  return W1,b1,W2,b2\n"
      ],
      "metadata": {
        "id": "88R18Cs9JNtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1,b1,W2,b2 = gradient_descent(X_train,y_train,100,0.01)"
      ],
      "metadata": {
        "id": "iiwt7QwSKwgl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}